{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contexto y Necesidad del proyecto**\n",
    "\n",
    "En nuestro entorno laboral, se generan PDFs diariamente con información relevante que incluye enlaces a páginas web, imágenes y redes sociales. Tradicionalmente, se ha utilizado un script monolítico para realizar todas las tareas de extracción de texto de las paginas web y de imagenes, lo que dificulta el mantenimiento, la ampliación y la escalabilidad del sistema. Este proyecto surge para:\n",
    "\n",
    "- **Automatizar el flujo de trabajo:** Permitir que cualquier usuario suba un PDF y obtenga de forma automática un .json con todo el texto extraido.\n",
    "- **Optimizar el procesamiento:** Utilizar técnicas de caché para evitar reprocesamientos, lo que reduce la carga en el sistema y mejora la eficiencia.\n",
    "- **Preparar la integración con sistemas avanzados:** Los resultados (almacenados en formatos CSV y JSON) servirán como insumo para procesos de generación de embeddings y para la creación de un agente especializado RAG, permitiendo responder a consultas complejas basadas en la información extraída.\n",
    "- **Facilitar futuras escalabilidades:** Una arquitectura modular, con cada componente definido a través de interfaces claras, permite que, si es necesario, se migre a una arquitectura de microservicios de manera sencilla sin reescribir la lógica de negocio.\n",
    "\n",
    "**Tareas Principales del Proyecto**\n",
    "\n",
    "1. **Extracción de URLs desde el PDF:**  \n",
    "   - **Objetivo:** Abrir el PDF diario y extraer todos los enlaces junto con el contexto (ubicación en el documento y fragmentos de texto cercanos).  \n",
    "   - **Salida:** Guardar los enlaces extraídos en un archivo CSV para facilitar su seguimiento.\n",
    "\n",
    "2. **Clasificación de URLs:**  \n",
    "   - **Objetivo:** Validar y clasificar los enlaces extraídos en distintas categorías:  \n",
    "     - Páginas HTML  \n",
    "     - Enlaces a imágenes  \n",
    "     - Enlaces a redes sociales u otros  \n",
    "   - **Salida:** Separar las URLs según su tipo para procesarlas de forma especializada.\n",
    "\n",
    "3. **Scraping de contenido HTML:**  \n",
    "   - **Objetivo:** Realizar solicitudes HTTP (o usar Selenium cuando sea necesario) a las URLs de páginas HTML.  \n",
    "   - **Tareas Específicas:**  \n",
    "     - Utilizar siempre el header `'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'` para garantizar la consistencia con la ejecución diaria.  \n",
    "     - Limpiar y extraer el contenido textual y metadatos (título, descripción).  \n",
    "     - Calcular la relevancia del contenido en base a palabras clave configuradas.  \n",
    "     - Implementar un sistema de caché, basado en una clave generada (por ejemplo, MD5 de la URL), que verifique de manera uniforme la expiración (24 horas o 86400 segundos) para evitar reprocesamientos innecesarios.\n",
    "\n",
    "4. **Extracción de texto de imágenes:**  \n",
    "   - **Objetivo:** Descargar las imágenes referenciadas en las URLs clasificadas como imágenes y extraer el texto que contienen mediante una API externa.  \n",
    "   - **Tareas Específicas:**  \n",
    "     - Aplicar un sistema de caché similar al del scraping HTML, utilizando una clave generada a partir de la imagen, para evitar llamadas redundantes a la API si el resultado ya está cacheado y es reciente.\n",
    "\n",
    "5. **Consolidación y seguimiento de resultados:**  \n",
    "   - **Objetivo:** Unificar los resultados obtenidos del scraping HTML y la extracción de texto de imágenes en un formato JSON consistente, que posteriormente servirá como insumo para la generación de embeddings y la creación de un agente RAG.  \n",
    "   - **Tareas Específicas:**  \n",
    "     - Actualizar un historial de URLs procesadas en un módulo independiente para evitar duplicidades en ejecuciones futuras, lo que favorece la reutilización y la escalabilidad.  \n",
    "     - Generar estadísticas de procesamiento (por ejemplo, total de URLs, porcentaje de contenido relevante) para monitorear y evaluar el rendimiento del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyecto cuyo objetivo es desarrollar un aplicativo web que permita a los usuarios de mi centro de trabajo subir un PDF (generado diariamente) y extraer de él enlaces, que se utilizarán para las siguientes tareas:\n",
    "\n",
    "1. **Extracción de URLs desde el PDF:**  \n",
    "   - Abrir el PDF y extraer todos los enlaces, junto con el contexto y la ubicación en el documento.  \n",
    "   - Guardar los enlaces extraídos en un archivo CSV para su seguimiento.\n",
    "\n",
    "2. **Clasificación de URLs:**  \n",
    "   - Validar y clasificar los enlaces extraídos en diferentes categorías (por ejemplo, páginas HTML, imágenes, redes sociales u otros).  \n",
    "   - Separar las URLs de imágenes de las URLs de páginas HTML para procesarlas de manera independiente.\n",
    "\n",
    "3. **Scraping de contenido HTML:**  \n",
    "   - Realizar solicitudes HTTP (o usar Selenium cuando sea necesario) a las URLs de páginas HTML, utilizando siempre el header `'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'` para asegurar la coherencia con la ejecución diaria.  \n",
    "   - Limpiar y extraer el contenido textual y metadatos relevantes (como título y descripción).  \n",
    "   - Calcular la relevancia del contenido basado en palabras clave.  \n",
    "   - Implementar un sistema de caché basado en una clave generada (por ejemplo, mediante un hash MD5 de la URL) y verificar la expiración de forma uniforme (24 horas, es decir, 86400 segundos) para evitar reprocesamientos.\n",
    "\n",
    "4. **Extracción de texto de imágenes:**  \n",
    "   - Descargar las imágenes referenciadas en las URLs clasificadas como imágenes.  \n",
    "   - Enviar las imágenes descargadas a una API externa para extraer el texto que contienen.  \n",
    "   - Aplicar una estrategia de caché similar a la del scraping HTML (usando una clave generada a partir de la imagen) y verificar la expiración de forma uniforme para evitar llamadas innecesarias a la API.\n",
    "\n",
    "5. **Consolidación y seguimiento de resultados:**  \n",
    "   - Unificar los resultados extraídos del scraping HTML y la extracción de texto de imágenes en un formato consistente (JSON) que servirá como insumo para generar embeddings y construir un agente especializado basado en técnicas RAG.  \n",
    "   - Actualizar el historial de URLs procesadas en un módulo independiente (por ejemplo, `history_tracker.py`) para evitar duplicidades en futuras ejecuciones, lo cual favorece la reutilización y escalabilidad.  \n",
    "   - Generar estadísticas de procesamiento (por ejemplo, cantidad total de URLs, porcentaje de contenido relevante, etc.) para monitoreo.\n",
    "\n",
    "Actualmente, dispongo de un script monolítico (py.txt) que implementa estas funciones, pero necesito modularizarlo de acuerdo a la siguiente estructura de proyecto:\n",
    "\n",
    "```\n",
    "scrap_1403/\n",
    "│\n",
    "├── codigo/                        \n",
    "│   ├── main.py                    # Orquestador principal: integra todo el flujo (basado en py)\n",
    "│   ├── lib/                       # Biblioteca de funciones modulares\n",
    "│   │   ├── config_manager.py      # Gestión de configuración (load_config, get_paths)\n",
    "│   │   ├── pdf_processor.py       # Extracción de enlaces y contexto desde PDFs; guardado en CSV\n",
    "│   │   ├── url_manager.py         # Validación y clasificación de URLs (páginas HTML, imágenes, social, etc.)\n",
    "│   │   ├── html_scraper.py        # Scraping de contenido HTML con caché y cálculo de relevancia (usando el User-Agent especificado)\n",
    "│   │   ├── image_processor.py     # Descarga de imágenes y extracción de texto de imágenes mediante API externa, con caché\n",
    "│   │   ├── api_client.py          # Cliente para la API externa de extracción de texto de imágenes\n",
    "│   │   ├── cache_utils.py         # Funciones para generación, guardado y carga de caché (clave MD5, expiración uniforme)\n",
    "│   │   ├── file_manager.py        # Gestión de archivos: lectura/escritura de CSV y JSON, creación de directorios\n",
    "│   │   └── history_tracker.py     # Seguimiento y actualización del historial de URLs procesadas (módulo independiente)\n",
    "│   │\n",
    "│   └── tests/                     # Pruebas unitarias para cada módulo\n",
    "│       ├── test_config_manager.py   \n",
    "│       ├── test_pdf_processor.py  \n",
    "│       ├── test_url_manager.py    \n",
    "│       ├── test_html_scraper.py   \n",
    "│       ├── test_image_processor.py\n",
    "│       ├── test_api_client.py     \n",
    "│       ├── test_cache_utils.py    \n",
    "│       ├── test_file_manager.py   \n",
    "│       └── test_history_tracker.py\n",
    "│\n",
    "├── credentials/                   # Credenciales y secretos\n",
    "│   ├── api_keys.yaml              # Claves de API (ejemplo, para la API de extracción de texto de imágenes)\n",
    "│   ├── credentials.env            # Archivo de variables de entorno (opcional)\n",
    "│   └── .gitignore                 # Configurado para excluir credenciales y archivos sensibles\n",
    "│\n",
    "├── base/                          # Carpeta de PDFs diarios para procesar\n",
    "│   └── [fecha].pdf                # Ejemplo: 01042025.pdf, 27022025.pdf, etc.\n",
    "│\n",
    "├── input/                         # Carpeta principal de datos procesados\n",
    "│   ├── In/                        # Enlaces extraídos de los PDFs\n",
    "│   │   └── links_extracted_[fecha].csv  # CSV con los enlaces extraídos de cada PDF\n",
    "│   │\n",
    "│   ├── Out/                       # Resultados del scraping\n",
    "│   │   ├── scraped_texts_[fecha].json   # Textos extraídos de páginas HTML\n",
    "│   │   └── Img_[fecha].json             # Textos extraídos de imágenes mediante API\n",
    "│   │\n",
    "│   ├── Images/                    # Datos de imágenes\n",
    "│   │   ├── image_links_[fecha].json     # Metadatos de enlaces de imágenes extraídos del PDF\n",
    "│   │   └── downloads/             # Imágenes descargadas organizadas por fecha\n",
    "│   │       └── [fecha]/           # Carpeta para cada fecha\n",
    "│   │           ├── img_1_[fecha].jpg    # Ejemplo: imagen descargada 1 del día\n",
    "│   │           ├── img_2_[fecha].jpg    # Ejemplo: imagen descargada 2 del día\n",
    "│   │           └── texto_imagenes_api.json  # Resultados del texto extraído de imágenes por API\n",
    "│   │\n",
    "│   ├── Social/                    # Datos relacionados con redes sociales (si se extraen)\n",
    "│   │   └── social_links_[fecha].json    # Enlaces a redes sociales extraídos del PDF\n",
    "│   │\n",
    "│   └── Stats/                     # Estadísticas del procesamiento diario\n",
    "│       └── stats_[fecha].json     # Estadísticas y métricas del día\n",
    "│\n",
    "└── requirements.txt               # Lista de dependencias para el proyecto (ej., PyMuPDF, Flask/FastAPI, requests, Selenium, etc.)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC:\n",
    "Se trabaja en una PC windows 11, en lenguaje python y se escribe el codigo en VS Code\n",
    "Prompt\n",
    "\n",
    "\"Eres un desarrollador experto en Python y arquitecturas de software escalables. Tengo un proyecto cuyo objetivo es desarrollar un aplicativo web que permita a los usuarios de mi centro de trabajo subir un PDF (generado diariamente) y extraer de él enlaces que se usarán para las siguientes tareas:\n",
    "\n",
    "1. **Extracción de URLs desde el PDF:**  \n",
    "   - Abrir el PDF y extraer todos los enlaces, junto con el contexto y la ubicación en el documento.  \n",
    "   - Guardar los enlaces extraídos en un archivo CSV para su seguimiento.\n",
    "\n",
    "2. **Clasificación de URLs:**  \n",
    "   - Validar y clasificar los enlaces extraídos en diferentes categorías (por ejemplo, HTML, imágenes, redes sociales u otros).  \n",
    "   - Separar las URLs de imágenes y de páginas HTML para procesarlas por separado.\n",
    "\n",
    "3. **Scraping de contenido HTML:**  \n",
    "   - Realizar solicitudes HTTP (o usar Selenium en casos necesarios) a las URLs de páginas HTML, utilizando siempre el header `'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'` para mantener la coherencia con la ejecución diaria.  \n",
    "   - Limpiar y extraer el contenido textual y metadatos relevantes (como título y descripción).  \n",
    "   - Calcular la relevancia del contenido basado en palabras clave.  \n",
    "   - Implementar un sistema de caché eficiente basado en una clave generada (por ejemplo, mediante un hash MD5 de la URL), y verificar de forma uniforme la expiración del caché (definida en 86400 segundos, es decir, 24 horas) para evitar reprocesamientos dentro de ese período.\n",
    "\n",
    "4. **Extracción de texto de imágenes:**  \n",
    "   - Descargar las imágenes referenciadas en las URLs clasificadas como imágenes.  \n",
    "   - Enviar las imágenes descargadas a una API externa para extraer el texto que contienen.  \n",
    "   - Aplicar una estrategia de caché similar a la del scraping HTML (utilizando una clave generada a partir de la imagen) y verificar la expiración uniformemente para evitar llamadas innecesarias a la API.\n",
    "\n",
    "5. **Consolidación y seguimiento de resultados:**  \n",
    "   - Unificar los resultados extraídos del scraping HTML y de la extracción de texto de imágenes en un formato JSON consistente, optimizado para su posterior uso en la generación de embeddings y construcción de un agente especializado basado en técnicas RAG.  \n",
    "   - Actualizar el historial de URLs procesadas en un módulo independiente (por ejemplo, `history_tracker.py`), lo que favorecerá la reutilización, las pruebas unitarias y la escalabilidad futura.  \n",
    "   - Generar estadísticas de procesamiento (por ejemplo, cantidad total de URLs, porcentaje de contenido relevante, etc.) para su monitoreo.\n",
    "\n",
    "El objetivo principal de esta refactorización es crear una arquitectura modular con componentes desacoplados mediante interfaces claras, lo que permitirá evolucionar fácilmente hacia una arquitectura de microservicios si las necesidades de escalabilidad lo requieren. El sistema debe permitir tanto la ejecución automática programada como la iniciada por usuarios a través de una interfaz web simple, donde cualquier usuario pueda subir un PDF y obtener automáticamente el JSON con todo el texto extraído.\n",
    "\n",
    "Actualmente dispongo de un script monolítico (main4_g2.py) que implementa todas estas funciones, pero necesito modularizarlo de acuerdo a la siguiente estructura de proyecto:\n",
    "\n",
    "```\n",
    "scrap_1402/\n",
    "│\n",
    "├── codigo/\n",
    "│   ├── main.py                    # Orquestador principal: integra todo el flujo (basado en main4_g2.py)\n",
    "│   ├── lib/\n",
    "│   │   ├── config_manager.py      # Gestión de configuración (load_config, get_paths)\n",
    "│   │   ├── pdf_processor.py       # Extracción de enlaces y contexto desde PDFs; guardado en CSV\n",
    "│   │   ├── url_manager.py         # Validación y clasificación de URLs (HTML, imágenes, social, etc.)\n",
    "│   │   ├── html_scraper.py        # Scraping de contenido HTML con caché y cálculo de relevancia (usando el User-Agent especificado)\n",
    "│   │   ├── image_processor.py     # Descarga de imágenes y extracción de texto mediante API externa, con caché\n",
    "│   │   ├── api_client.py          # Cliente para la API externa de extracción de texto de imágenes\n",
    "│   │   ├── cache_utils.py         # Funciones para generación, guardado y carga de caché (clave MD5, expiración uniforme)\n",
    "│   │   ├── file_manager.py        # Gestión de archivos: lectura/escritura de CSV y JSON, creación de directorios\n",
    "│   │   └── history_tracker.py     # Seguimiento y actualización del historial de URLs procesadas (módulo independiente)\n",
    "│   │\n",
    "│   └── tests/                     # Pruebas unitarias para cada módulo\n",
    "│       ├── test_config_manager.py   \n",
    "│       ├── test_pdf_processor.py  \n",
    "│       ├── test_url_manager.py    \n",
    "│       ├── test_html_scraper.py   \n",
    "│       ├── test_image_processor.py\n",
    "│       ├── test_api_client.py     \n",
    "│       ├── test_cache_utils.py    \n",
    "│       ├── test_file_manager.py   \n",
    "│       └── test_history_tracker.py\n",
    "│\n",
    "├── credentials/                   # Credenciales y secretos\n",
    "│   ├── api_keys.yaml              # Claves de API (ejemplo, para la API de extracción de texto de imágenes)\n",
    "│   ├── credentials.env            # Archivo de variables de entorno (opcional)\n",
    "│   └── .gitignore                 # Configurado para excluir credenciales y archivos sensibles\n",
    "│\n",
    "├── base/                          # Carpeta de PDFs diarios para procesar\n",
    "│   └── [fecha].pdf                # Ejemplo: 01042025.pdf, 27022025.pdf, etc.\n",
    "│\n",
    "├── input/                         # Carpeta principal de datos procesados\n",
    "│   ├── In/                        # Enlaces extraídos de los PDFs\n",
    "│   │   └── links_extracted_[fecha].csv  # CSV con los enlaces extraídos de cada PDF\n",
    "│   │\n",
    "│   ├── Out/                       # Resultados del scraping\n",
    "│   │   ├── scraped_texts_[fecha].json   # Textos extraídos de páginas HTML\n",
    "│   │   └── Img_[fecha].json             # Textos extraídos de imágenes mediante API\n",
    "│   │\n",
    "│   ├── Images/                    # Datos de imágenes\n",
    "│   │   ├── image_links_[fecha].json     # Metadatos de enlaces de imágenes extraídos del PDF\n",
    "│   │   └── downloads/             # Imágenes descargadas organizadas por fecha\n",
    "│   │       └── [fecha]/           # Carpeta para cada fecha\n",
    "│   │           ├── img_1_[fecha].jpg    # Ejemplo: imagen descargada 1 del día\n",
    "│   │           ├── img_2_[fecha].jpg    # Ejemplo: imagen descargada 2 del día\n",
    "│   │           └── texto_imagenes_api.json  # Resultados de la extracción de texto de imágenes por API\n",
    "│   │\n",
    "│   ├── Social/                    # Datos relacionados con redes sociales (si se extraen)\n",
    "│   │   └── social_links_[fecha].json    # Enlaces a redes sociales extraídos del PDF\n",
    "│   │\n",
    "│   └── Stats/                     # Estadísticas del procesamiento diario\n",
    "│       └── stats_[fecha].json     # Estadísticas y métricas del día\n",
    "│\n",
    "└── requirements.txt               # Lista de dependencias para el proyecto (ej., PyMuPDF, Flask/FastAPI, requests, Selenium, etc.)\n",
    "```\n",
    "\n",
    "Ten en cuenta que:\n",
    "\n",
    "- La configuración debe incluir un parámetro `cache_expiry` de 86400 (24 horas en segundos) para determinar el tiempo de validez del caché.\n",
    "- En el módulo de scraping HTML se debe usar el header `'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'` para reemplazar cualquier valor genérico o por defecto, manteniendo así la coherencia con la ejecución diaria.\n",
    "- Tanto los módulos de scraping HTML como el de extracción de texto de imágenes deben implementar un sistema de caché uniforme (con funciones en `cache_utils.py`) basado en una clave generada (por ejemplo, mediante un hash MD5) y verificar la expiración de forma consistente en ambos, para evitar reprocesamientos dentro del período definido y maximizar la eficiencia.\n",
    "- La actualización del historial de URLs procesadas debe implementarse en un módulo independiente (por ejemplo, `history_tracker.py`), ya que esto favorece la reutilización, el testing y la escalabilidad futura.\n",
    "- La modularización debe preservar la funcionalidad existente del script monolítico (main4_g2.py), separándola en módulos independientes según la estructura propuesta, de modo que el sistema actual siga funcionando sin cambios en su salida.\n",
    "- Cada módulo debe tener interfaces claras y bien documentadas (por ejemplo, funciones o clases con entradas y salidas definidas) que faciliten la extracción y el despliegue independiente, permitiendo que en el futuro se migre a microservicios si se requiere escalar el sistema.\n",
    "- Diseñar cada módulo con capacidad de procesamiento asíncrono y paralelo cuando sea posible, para maximizar el rendimiento cuando se procesen PDFs con gran cantidad de enlaces.\n",
    "- La estructura JSON de salida debe optimizarse considerando su futuro uso para generar embeddings y construir un agente RAG especializado, asegurando que los metadatos y el contenido extraído sean adecuados para este propósito.\n",
    "\n",
    "Por favor, refactoriza el código del script monolítico (main4_g2.py) dividiéndolo en los módulos indicados (config_manager.py, pdf_processor.py, url_manager.py, html_scraper.py, image_processor.py, api_client.py, cache_utils.py, file_manager.py y history_tracker.py) y crea un orquestador principal en main.py que coordine todo el flujo (desde la extracción del PDF, clasificación de URLs, scraping HTML, extracción de texto de imágenes hasta la consolidación de resultados). Además, genera un archivo .gitignore que excluya credenciales, caché, logs y entornos virtuales.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
